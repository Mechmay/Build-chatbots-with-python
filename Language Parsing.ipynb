{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover insights into classic texts\n",
    "<div data-testid=\"markdown\" class=\"spacing-tight__2Gp7GTqG0TykPQ18OnUOVt markdown__1eeYJ4WPKUcvX_LDDGJR12\"><p class=\"p__1qg33Igem5pAgn4kPMirjw\">Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.</p>\n",
    "<p class=\"p__1qg33Igem5pAgn4kPMirjw\">In this project you will perform a natural language parsing analysis to gain deeper insight into one of two famous and often discussed novels in the public domain: <a href=\"http://www.gutenberg.org/ebooks/174\" target=\"_blank\" rel=\"noopener\" class=\"gamut-15hd59n-Anchor e14vpv2g0\">Oscar Wilde’s <em>The Picture of Dorian Gray</em></a> or <a href=\"http://www.gutenberg.org/ebooks/6130\" target=\"_blank\" rel=\"noopener\" class=\"gamut-15hd59n-Anchor e14vpv2g0\">Homer’s <em>The Iliad!</em></a> Fear not if you haven’t heard or read the novels, one of the beauties of natural language parsing with regular expressions is the ability to gain insight into lengthy pieces of text without a formal read!</p>\n",
    "<p class=\"p__1qg33Igem5pAgn4kPMirjw\">By the end of this project, you will find out the main topics of discussion in the novel of your choosing and can begin to discern some of the author’s thoughts and beliefs!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'iliad', 'of', 'homer', 'translated', 'by', 'alexander', 'pope', ',', 'with', 'notes', 'by', 'the', 'rev', '.', 'theodore', 'alois', 'buckley', ',', 'm.a.', ',', 'f.s.a', '.', 'and', 'flaxman', \"'s\", 'designs', '.']\n",
      "[('the', 'DT'), ('iliad', 'NN'), ('of', 'IN'), ('homer', 'NN'), ('translated', 'VBN'), ('by', 'IN'), ('alexander', 'NN'), ('pope', 'NN'), (',', ','), ('with', 'IN'), ('notes', 'NNS'), ('by', 'IN'), ('the', 'DT'), ('rev', 'NN'), ('.', '.'), ('theodore', 'NN'), ('alois', 'NN'), ('buckley', 'NN'), (',', ','), ('m.a.', 'NN'), (',', ','), ('f.s.a', 'NN'), ('.', '.'), ('and', 'CC'), ('flaxman', 'NN'), (\"'s\", 'POS'), ('designs', 'NNS'), ('.', '.')]\n",
      "\n",
      "\n",
      " [((('hector', 'NN'),), 322), ((('i', 'NN'),), 277), ((('jove', 'NN'),), 257), ((('troy', 'NN'),), 208), ((('vain', 'NN'),), 195), ((('war', 'NN'),), 193), ((('son', 'NN'),), 170), ((('thou', 'NN'),), 158), ((('the', 'DT'), ('plain', 'NN')), 157), ((('the', 'DT'), ('field', 'NN')), 154), ((('the', 'DT'), ('ground', 'NN')), 138), ((('death', 'NN'),), 134), ((('hand', 'NN'),), 134), ((('greece', 'NN'),), 128), ((('heaven', 'NN'),), 127), ((('fate', 'NN'),), 127), ((('thee', 'NN'),), 122), ((('breast', 'NN'),), 121), ((('the', 'DT'), ('trojan', 'NN')), 120), ((('the', 'DT'), ('god', 'NN')), 119), ((('the', 'DT'), ('war', 'NN')), 117), ((('the', 'DT'), ('greeks', 'NN')), 116), ((('blood', 'NN'),), 115), ((('homer', 'NN'),), 112), ((('the', 'DT'), ('king', 'NN')), 105), ((('rage', 'NN'),), 103), ((('force', 'NN'),), 103), ((('care', 'NN'),), 99), ((('head', 'NN'),), 98), ((('man', 'NN'),), 97)]\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "\n",
    "# import text of choice here\n",
    "text = open('dorian_gray.txt', encoding= 'utf-8').read().lower()\n",
    "text1 = open('the_iliad.txt', encoding= 'utf-8').read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text1)\n",
    "# print(word_tokenized_text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text[0]\n",
    "print(single_word_tokenized_sentence)\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for word in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "  pos_tagged_text.append(pos_tag(word))\n",
    "\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[0]\n",
    "print(single_pos_sentence)\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "np_chunk_grammar = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = 'VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}'\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "for tag in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "  np_chunked_text.append(np_chunk_parser.parse(tag))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(tag))\n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print('\\n\\n',most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis for The Picture of Dorian Gray\n",
    "\n",
    "Looking at most_common_np_chunks, you can identify characters of importance in the text such as henry, harry, dorian gray, and basil, based on their frequency. Additionally another noun phrase the picture appears to be very relevant.\n",
    "\n",
    "##### Analysis for The Iliad\n",
    "\n",
    "Looking at most_common_np_chunks, you can identify characters of importance in the text such as hector and jove based on their frequency. Additionally a location of importance, troy, is mentioned often. A theme of war can also implied by its high frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(((\"'t\", 'NN'), ('is', 'VBZ')), 19), ((('i', 'NN'), ('am', 'VBP')), 11), (((\"'t\", 'NN'), ('was', 'VBD')), 11), ((('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('saw', 'VBD')), 8), ((('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')), 7), ((('i', 'NN'), ('was', 'VBD')), 6), ((('confess', 'NN'), (\"'d\", 'VBD')), 6), ((('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')), 6), ((('view', 'NN'), (\"'d\", 'VBD')), 5), ((('i', 'NN'), ('felt', 'VBD')), 5), ((('i', 'NN'), ('bear', 'VBP')), 5), ((('hector', 'NN'), ('is', 'VBZ')), 5), ((('vain', 'NN'), ('was', 'VBD')), 5), ((('homer', 'NN'), ('was', 'VBD')), 4), ((('i', 'NN'), ('have', 'VBP')), 4), ((('hunger', 'NN'), ('was', 'VBD')), 4), ((('glory', 'NN'), ('lost', 'VBN')), 4), ((('i', 'NN'), ('see', 'VBP')), 4), ((('war', 'NN'), ('be', 'VB')), 4), ((('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')), 4), ((('i', 'NN'), ('go', 'VBP')), 4), ((('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')), 4), ((('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')), 4), ((('father', 'NN'), ('gave', 'VBD')), 4), ((('i', 'NN'), ('deem', 'VBP')), 4), ((('minerva', 'NN'), ('repressing', 'VBG')), 3), ((('thetis', 'NN'), ('calling', 'VBG')), 3), ((('thetis', 'NN'), ('entreating', 'VBG')), 3)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis for The Picture of Dorian Gray\n",
    "\n",
    "Looking at most_common_vp_chunks, some interesting findings appear. The verb phrases i want, i know and i have occur frequently, indicating a theme of desire and need.\n",
    "\n",
    "##### Analysis for The Iliad\n",
    "\n",
    "Looking at most_common_vp_chunks, you can see that verb phrases of the form you defined in your chunk grammar do not appear as often in The Iliad as noun phrases. This can indicate a different style of writing taken by the author that does not follow traditional grammatical style (i.e. poetry). Even when chunks are not found, their absence can give you insight!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
